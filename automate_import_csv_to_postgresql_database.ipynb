{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2 in c:\\users\\tanim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.9.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChicagoCensusData.csv', 'ChicagoCrimeData.csv', 'ChicagoPublicSchools.csv']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find csv files in my current working directory and isolate them\n",
    "import contextlib\n",
    "csv_files = [file for file in os.listdir(os.getcwd()) if file.endswith(\"csv\")]\n",
    "csv_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a new directory and move them There\n",
    "dataset_dir = \"datasets\"\n",
    "with contextlib.suppress(Exception):\n",
    "    mkdir = f\"mkdir {dataset_dir}\"\n",
    "    os.system(mkdir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.suppress(Exception):\n",
    "    for csv in csv_files:\n",
    "        shutil.move(csv,dataset_dir)\n",
    "       \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pandas dataframe from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f'{os.getcwd()}/{dataset_dir}/'\n",
    "try:\n",
    "    df = {file: pd.read_csv(data_path+file) for file in csv_files}\n",
    "except UnicodeDecodeError:\n",
    "    df = {file: pd.read_csv(data_path+file,encoding=\"ISO-8859-1\") for file in csv_files}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean tables names and column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in csv_files:\n",
    "    dataframe=df[k]\n",
    "    clean_tbl_name=k.lower().replace(\" \",\"_\").replace(\"?\",\"\") \\\n",
    "        .replace(\"-\",\"_\").replace(r\"/\",\"_\").replace(\"\\\\\",\"_\").replace(\"%\",\"_\") \\\n",
    "            .replace(\")\",\"\").replace(\",\",\"\").replace(\"$\",\"\")\n",
    "    #remove.csv extension\n",
    "    tbl_name = f'{clean_tbl_name.split(\".\")[0]}'\n",
    "    \n",
    "    dataframe.columns = [x.lower().replace(\" \", \"_\").replace(\"?\", \"\")\n",
    "                         .replace(\"-\", \"_\").replace(r\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\"%\", \"_\") \\\n",
    "                             .replace(\"(\",\"\")\n",
    "                         .replace(\")\", \"\").replace(\",\", \"\").replace(\"$\", \"\") for x in dataframe.columns]\n",
    "\n",
    "    #replacement dictionary that maps pandas dtypes to sql dtypes\n",
    "    replacements={\n",
    "        \"object\": \"varchar\",\n",
    "        \"float64\": \"float\",\n",
    "        \"int64\": \"int\",\n",
    "        \"datetime64\": \"timestamp\",\n",
    "        \"timedelta64\": \"varchar\",\n",
    "        \"string\": \"varchar\",\n",
    "\n",
    "    }\n",
    "\n",
    "    #table schema\n",
    "    col_str = \", \".join(f\"{n} {d}\" for (n, d) in zip(dataframe.columns, dataframe.dtypes.replace(replacements)))\n",
    "    #adding the db connection\n",
    "    host = \"127.0.0.1\"\n",
    "    port = \"5432\"\n",
    "    dbname = \"chicago_data\"\n",
    "    user = \"postgres\"\n",
    "    password = \"Candycove456\"\n",
    "    conn_string = f\"host={host} dbname={dbname} user={user} password={password}\"\n",
    "    conn = psycopg2.connect(conn_string)\n",
    "    cursor = conn.cursor()\n",
    "    #drop table with same name\n",
    "    cursor.execute(f\"drop table if exists {tbl_name};\")\n",
    "  \n",
    "    #create table\n",
    "    cursor.execute(\"create table %s (%s);\" % (tbl_name, col_str))\n",
    "    #insert values to table\n",
    "    dataframe.to_csv(k,header=dataframe.columns,index=False,encoding=\"utf-8\")\n",
    "    my_file=open(k)\n",
    "    SQL_STATEMENT=\"\"\"\n",
    "    COPY %s FROM STDIN WITH \n",
    "    CSV \n",
    "    HEADER \n",
    "     DELIMITER AS ','\n",
    "    \"\"\"\n",
    "    cursor.copy_expert(sql=SQL_STATEMENT % tbl_name,file=my_file)\n",
    "    cursor.execute(\"grant select on table %s to public\" % tbl_name)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7e6d8ab07031a4324fb522b039479764876aa1eb04f40bfc46e2fdde2fc89f6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
